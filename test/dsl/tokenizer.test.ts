// SPDX-License-Identifier: GPL-3.0-or-later
// Copyright (c) 2023 Levi Gruspe
// Test tokenizer.ts.

import { Tag, Token, tokenize } from "../../src/dsl/tokenizer";

import { strict as assert } from "assert";

/**
 * Extract literals from results of `tokenize()`.
 */
function literals(tokens: Token[]): string[] {
  return tokens.map((token) => token.literal);
}

describe("tokenize", () => {
  it("should not insert spurious new lines at the end", () => {
    assert.deepEqual([], literals(tokenize("")));
    assert.deepEqual(["foo"], literals(tokenize("foo")));
  });

  it("should preserve new lines in code", () => {
    assert.deepEqual(["\n"], literals(tokenize("\n")));
    assert.deepEqual(["foo", "\n"], literals(tokenize("foo\n")));
  });

  it("should ignore comments", () => {
    assert.deepEqual([], literals(tokenize("-- test")));
    assert.deepEqual([], literals(tokenize("--test")));
    assert.deepEqual([], literals(tokenize(" -- test")));
    assert.deepEqual([], literals(tokenize("  --test")));

    const code = `
      foo -- footest
      bar
      -- bartest
      baz
      -- baztest
    `;
    const expected = ["\n", "foo", "\n", "bar", "\n", "\n", "baz", "\n", "\n"];
    const tokens = tokenize(code);
    assert.deepEqual(expected, literals(tokens));
  });

  it("should not treat '#' as the start of a comment", () => {
    assert.equal(tokenize("# foo")[0].tag, Tag.Terminal);
    assert.equal(tokenize("#foo")[0].tag, Tag.Terminal);
  });

  describe("words", () => {
    describe("if word looks like a variable name", () => {
      it("should be tagged as a variable", () => {
        const examples = [
          "A",
          "B",
          "C",
          "Foo",
          "Bar",
          "Baz",
          "A1",
          "X1",
          "Y2",
          "Z3",
        ];
        for (const example of examples) {
          const tokens = tokenize(example);
          assert.equal(tokens.length, 1);

          const token = tokens[0];
          assert.equal(token.tag, Tag.Variable);
          assert.equal(token.literal, example);
        }
      });
    });

    describe("if word does not look like a variable name", () => {
      it("should be tagged as a terminal", () => {
        const examples = [
          "qwerty",
          "asdf",
          "foo",
          "bar",
          "baz",
          "AÃ¤", // not a variable, because it's not ASCII alphanumeric
        ];
        for (const example of examples) {
          const tokens = tokenize(example);
          assert.equal(tokens.length, 1);

          const token = tokens[0];
          assert.equal(token.tag, Tag.Terminal);
          assert.equal(token.literal, example);
        }
      });
    });

    describe("if word is an IPA segment", () => {
      it("should be tagged as a terminal", () => {
        const examples = [
          // Vowels
          "i",
          "y",
          "É¨",
          "Ê‰",
          "É¯",
          "u",
          "Éª",
          "Ê",
          "ÊŠ",
          "e",
          "Ã¸",
          "É˜",
          "Éµ",
          "É¤",
          "o",
          "eÌž",
          "Ã¸Ìž",
          "É™",
          "É¤Ìž",
          "oÌž",
          "É›",
          "Å“",
          "Éœ",
          "Éž",
          "ÊŒ",
          "É”",
          "Ã¦",
          "É",
          "a",
          "É¶",
          "Ã¤",
          "É‘",
          "É’",

          // Pulmonic consonants
          "mÌ¥",
          "m",
          "É±",
          "nÌ¼",
          "nÌ¥",
          "n",
          "É³ÌŠ",
          "É³",
          "É²ÌŠ",
          "É²",
          "Å‹ÌŠ",
          "Å‹",
          "É´",
          "p",
          "b",
          "pÌª",
          "bÌª",
          "tÌ¼",
          "dÌ¼",
          "t",
          "d",
          "Êˆ",
          "É–",
          "c",
          "ÉŸ",
          "k",
          "É¡",
          "q",
          "É¢",
          "Ê¡",
          "Ê”",
          "ts",
          "dz",
          "tÌ Êƒ",
          "dÌ Ê’",
          "tÊ‚",
          "dÊ",
          "tÉ•",
          "dÊ‘",
          "pÉ¸",
          "bÎ²",
          "pÌªf",
          "bÌªv",
          "tÌªÎ¸",
          "dÌªÃ°",
          "tÉ¹ÌÌŠ",
          "dÉ¹Ì",
          "tÌ É¹Ì ÌŠË”",
          "dÌ É¹Ì Ë”",
          "cÃ§",
          "ÉŸÊ",
          "kx",
          "É¡É£",
          "qÏ‡",
          "É¢Ê",
          "Ê¡Êœ",
          "Ê¡Ê¢",
          "Ê”h",
          "s",
          "z",
          "Êƒ",
          "Ê’",
          "Ê‚",
          "Ê",
          "É•",
          "Ê‘",
          "É¸",
          "Î²",
          "f",
          "v",
          "Î¸Ì¼",
          "Ã°Ì¼",
          "Î¸",
          "Ã°",
          "Î¸Ì ",
          "Ã°Ì ",
          "É¹Ì ÌŠË”",
          "É¹Ì Ë”",
          "É»ÌŠË”",
          "É»Ë”",
          "Ã§",
          "Ê",
          "x",
          "É£",
          "Ï‡",
          "Ê",
          "Ä§",
          "Ê•",
          "h",
          "É¦",
          "Ê‹",
          "É¹",
          "É»",
          "j",
          "É°",
          "Ê”Ìž",
          "â±±ÌŸ",
          "â±±",
          "É¾Ì¼",
          "É¾Ì¥",
          "É¾",
          "É½ÌŠ",
          "É½",
          "É¢Ì†",
          "Ê¡Ì†",
          "Ê™Ì¥",
          "Ê™",
          "rÌ¥",
          "r",
          "É½ÌŠrÌ¥",
          "É½r",
          "Ê€Ì¥",
          "Ê€",
          "Êœ",
          "Ê¢",
          "tÉ¬",
          "dÉ®",
          "têžŽ",
          "dð¼…",
          "cð¼†",
          "ÉŸÊŽÌ",
          "kð¼„",
          "É¡ÊŸÌ",
          "É¬",
          "É®",
          "êžŽ",
          "ð¼…",
          "ð¼†",
          "ÊŽÌ",
          "ð¼„",
          "ÊŸÌ",
          "l",
          "É­",
          "ÊŽ",
          "ÊŸ",
          "ÊŸÌ ",
          "ÉºÌ¥",
          "Éº",
          "ð¼ˆÌ¥",
          "ð¼ˆ",
          "ÊŽÌ†",
          "ÊŸÌ†",

          // Non-pulmonic consonants
          "pÊ¼",
          "tÊ¼",
          "ÊˆÊ¼",
          "cÊ¼",
          "kÊ¼",
          "qÊ¼",
          "Ê¡Ê¼",
          "tÌªÎ¸Ê¼",
          "tsÊ¼",
          "tÌ ÊƒÊ¼",
          "tÊ‚Ê¼",
          "kxÊ¼",
          "qÏ‡Ê¼",
          "É¸Ê¼",
          "fÊ¼",
          "Î¸Ê¼",
          "sÊ¼",
          "ÊƒÊ¼",
          "Ê‚Ê¼",
          "É•Ê¼",
          "xÊ¼",
          "Ï‡Ê¼",
          "tÉ¬Ê¼",
          "cð¼†Ê¼",
          "kð¼„Ê¼",
          "É¬Ê¼",
          "kÊ˜",
          "qÊ˜",
          "kÇ€",
          "qÇ€",
          "kÇƒ",
          "qÇƒ",
          "kð¼Š",
          "qð¼Š",
          "kÇ‚",
          "qÇ‚",
          "É¡Ê˜",
          "É¢Ê˜",
          "É¡Ç€",
          "É¢Ç€",
          "É¡Çƒ",
          "É¢Çƒ",
          "É¡ð¼Š",
          "É¢ð¼Š",
          "É¡Ç‚",
          "É¢Ç‚",
          "Å‹Ê˜",
          "É´Ê˜",
          "Å‹Ç€",
          "É´Ç€",
          "Å‹Çƒ",
          "É´Çƒ",
          "Å‹ð¼Š",
          "É´ð¼Š",
          "Å‹Ç‚",
          "É´Ç‚",
          "Êž",
          "kÇ",
          "qÇ",
          "É¡Ç",
          "É¢Ç",
          "Å‹Ç",
          "É´Ç",
          "É“",
          "É—",
          "á¶‘",
          "Ê„",
          "É ",
          "Ê›",
          "É“Ì¥",
          "É—Ì¥",
          "á¶‘ÌŠ",
          "Ê„ÌŠ",
          "É ÌŠ",
          "Ê›Ì¥",

          // Co-articulated consonants
          "nÍ¡m",
          "Å‹Í¡m",
          "tÍ¡p",
          "dÍ¡b",
          "kÍ¡p",
          "É¡Í¡b",
          "qÍ¡Ê¡",
          "É¥ÌŠ",
          "É¥",
          "Ê",
          "w",
          "É§",
          "É«",

          // Other consonants
          "jÌƒ",
          "wÌƒ",
          "hÌƒ",
          "tÌªÊ™Ì¥",
          "hÌªÍ†",

          // Tone registers
          "Ë¥",
          "êœ’",
          "Ë¦",
          "êœ“",
          "Ë§",
          "êœ”",
          "Ë¨",
          "êœ•",
          "Ë©",
          "êœ–",

          // Tone contours
          "Ë©Ë¥",
          "êœ–êœ’",
          "Ë¥Ë©",
          "êœ’êœ–",
          "Ë§Ë¥",
          "êœ”êœ’",
          "Ë¨Ë¦",
          "êœ•êœ“",
          "Ë©Ë§",
          "êœ–êœ”",
          "Ë¥Ë§",
          "êœ’êœ”",
          "Ë¦Ë¨",
          "êœ“êœ•",
          "Ë§Ë©",
          "êœ”êœ–",
          "Ë§Ë¥Ë¨",
          "Ë¨Ë¦Ë¨",
          "Ë©Ë§Ë©",
          "êœ”êœ’êœ•",
          "êœ•êœ“êœ•",
          "êœ–êœ”êœ–",
          "Ë¥Ë§Ë¥",
          "Ë¦Ë¨Ë¦",
          "Ë§Ë©Ë§",
          "êœ’êœ”êœ’",
          "êœ“êœ•êœ“",
          "êœ”êœ–êœ”",
        ];
        for (const example of examples) {
          const tokens = tokenize(example);
          assert.equal(tokens.length, 1);

          const token = tokens[0];
          assert.equal(token.tag, Tag.Terminal);
          assert.equal(token.literal, example);
        }
      });
    });

    it("should break words at reserved symbols", () => {
      const examples: Array<[string, string[]]> = [
        ["!foo!bar!baz!", ["!", "foo", "!", "bar", "!", "baz", "!"]],
        ["$foo$bar$baz$", ["$", "foo", "$", "bar", "$", "baz", "$"]],
        ["%foo%bar%baz%", ["%", "foo", "%", "bar", "%", "baz", "%"]],
        ["&foo&bar&baz&", ["&", "foo", "&", "bar", "&", "baz", "&"]],
        ["'foo'bar'baz'", ["'", "foo", "'", "bar", "'", "baz", "'"]],
        ["(foo(bar(baz(", ["(", "foo", "(", "bar", "(", "baz", "("]],
        [")foo)bar)baz)", [")", "foo", ")", "bar", ")", "baz", ")"]],
        ["*foo*bar*baz*", ["*", "foo", "*", "bar", "*", "baz", "*"]],
        ["+foo+bar+baz+", ["+", "foo", "+", "bar", "+", "baz", "+"]],
        [",foo,bar,baz,", [",", "foo", ",", "bar", ",", "baz", ","]],
        ["-foo-bar-baz-", ["-", "foo", "-", "bar", "-", "baz", "-"]],
        [":foo:bar:baz:", [":", "foo", ":", "bar", ":", "baz", ":"]],
        [";foo;bar;baz;", [";", "foo", ";", "bar", ";", "baz", ";"]],
        ["<foo<bar<baz<", ["<", "foo", "<", "bar", "<", "baz", "<"]],
        [">foo>bar>baz>", [">", "foo", ">", "bar", ">", "baz", ">"]],
        ["?foo?bar?baz?", ["?", "foo", "?", "bar", "?", "baz", "?"]],
        ["@foo@bar@baz@", ["@", "foo", "@", "bar", "@", "baz", "@"]],
        ["[foo[bar[baz[", ["[", "foo", "[", "bar", "[", "baz", "["]],
        ["\\foo\\bar\\baz\\", ["\\", "foo", "\\", "bar", "\\", "baz", "\\"]],
        ["]foo]bar]baz]", ["]", "foo", "]", "bar", "]", "baz", "]"]],
        ["^foo^bar^baz^", ["^", "foo", "^", "bar", "^", "baz", "^"]],
        ["`foo`bar`baz`", ["`", "foo", "`", "bar", "`", "baz", "`"]],
        ['"foo"bar"baz"', ['"', "foo", '"', "bar", '"', "baz", '"']],
      ];

      for (const [example, expected] of examples) {
        assert.deepEqual(literals(tokenize(example)), expected);
      }
    });

    it("should break words at separators", () => {
      const examples: Array<[string, string[]]> = [
        [".foo.bar.baz.", [".", "foo", ".", "bar", ".", "baz", "."]],
        ["/foo/bar/baz/", ["/", "foo", "/", "bar", "/", "baz", "/"]],
        ["=foo=bar=baz=", ["=", "foo", "=", "bar", "=", "baz", "="]],
        ["\nfoo\nbar\nbaz\n", ["\n", "foo", "\n", "bar", "\n", "baz", "\n"]],
        ["{foo{bar{baz{", ["{", "foo", "{", "bar", "{", "baz", "{"]],
        ["|foo|bar|baz|", ["|", "foo", "|", "bar", "|", "baz", "|"]],
        ["}foo}bar}baz}", ["}", "foo", "}", "bar", "}", "baz", "}"]],
        ["~foo~bar~baz~", ["~", "foo", "~", "bar", "~", "baz", "~"]],
      ];
      for (const [example, expected] of examples) {
        assert.deepEqual(literals(tokenize(example)), expected);
      }
    });

    it("should break words at '#'", () => {
      assert.deepEqual(literals(tokenize("#foo#bar#baz#")), [
        "#",
        "foo",
        "#",
        "bar",
        "#",
        "baz",
        "#",
      ]);
    });

    it("should break words at '_'", () => {
      assert.deepEqual(literals(tokenize("_foo_bar_baz_")), [
        "_",
        "foo",
        "_",
        "bar",
        "_",
        "baz",
        "_",
      ]);
    });
  });
});
